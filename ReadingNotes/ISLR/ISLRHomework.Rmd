---
title: "Homework"
author: "Wang Liuying"
output: pdf_document
---

####HW2
```{r}
set.seed (1)
y = rnorm(100)
x = rnorm(100)
y = x - 2 * x ^ 2 + rnorm(100)
plot(x, y)
mydata = data.frame(x, y)
set.seed(3)
fold = sample(rep(1:5, 20))
mse = rep(0, 4)
for (degree in 1:4) {
  yhat = rep(0, 100)
  for (i in 1:5) {
    train = (1:100)[fold != i]
    lm1.fit = lm(y ~ poly(x, degree), data = mydata, subset = train)
    yhat[-train] = predict(lm1.fit, mydata)[-train]
  }
  mse[degree] = mean((yhat - y) ^ 2)
}
plot(1:4, mse)

```

a)	5-fold cross validation.

b) I would choose the degree of 2. Accroding to 5-fold CV this model has the (almost) the least MSE for test error. Degree of 3 or 4 shows almost the same MSE but we finally choose degree of 2 for it has the least complexity.

c) 
```{r}
library(leaps)
model.fit <- step(lm(formula=y~x+I(x^2)+I(x^3)+I(x^4)),data=mydata)
```

Yes, model of degree 2 survive the backward stepwise selection.


####HW4 - (i) Using R to generate Figure 4.4 and provide your codes. 
```{r}
set.seed(3)
x <- seq(-4,4,by=0.1)
den.1 <- dnorm(x,mean=-1.25,sd=1)
den.2 <- dnorm(x,mean=1.25,sd=1)
x1 <- rnorm(20,mean=-1.25,sd=1)
x2 <- rnorm(20,mean=1.25,sd=1)
par(mfrow=c(1,2))
plot(x,den.1,type="l",col="green",xlim=c(-4,4))
lines(x,den.2,col="purple")
library(MASS)
plotrix::multhist(list(x1,x2),beside=F,col=c("green","purple"))
```

####Exercise 5

a. If the Bayes decision boundary is linear, LDA model will perform nice 
on the training set ,and QDA will be no better than LDA, where the improvement
on bias is quite limited and no way to reduce variance of estimation. As for 
test set, we expect LDA perform better, since it makes the proper assumption, and 
uses more information.

b. It depends. If the sample size is large enough, QDA should perform better than 
LDA since it is closer to bayes decision boundary. However, QDA has much more 
parameters to be estimated, if the boudary is not so far away from linear, and the sample
size is small, QDA may have quite large variance, especially on the test set.

c. we expect the test prediction accuracy of QDA relative to LDA to improve,
because we are not bothered about the sample size any more. As the sample size
increases, the variance will decrease to an acceptable level.

d. False. That may happen on training set but it accually implies over-fitting. 


####Exercise 6

$logit(p)=\beta_0+\beta_1x_1+\beta_2x2$,
$p=\frac{e^{\beta_0+\beta_1x_1+\beta_2x2}}{1+e^{\beta_0+\beta_1x_1+\beta_2x2}}$

```{r}
exp(-6 + 0.05 * 40 + 1 * 3.5)/(1 + exp(-6 + 0.05 * 40 + 1 * 3.5))
```

$logit(p)=log(\frac{p}{1-p})=\beta_0+\beta_1x_1+\beta_2x2$
```{r}
logit <- log(0.5/(1 - 0.5))
(logit + 6 - 1 * 3.5) / (0.05)
```

####Exercise 9

$odds=\frac{p}{1-p}$, thus if $odds=0.37$

$p=\frac{odds}{1+odds}=\frac{0.37}{1+0.37}=0.27$

if $p=16\%$,

then $odds=\frac{0.16}{1-0.16}=0.19$

####Exercise 11
```{r}
library(ISLR)
data(Auto)

head(Auto)
Auto$mpg01=ifelse(Auto$mpg>=median(Auto$mpg),1,0)#create binary variable
library(car)
par(mfrow=c(2,3))
for (i in 2:7)
{
  Boxplot(y=Auto[,i],g=Auto$mpg01,ylab=names(Auto[i]))
}
par(mfrow=c(1,1))
plot(mpg01~origin,data=Auto)
```
thus "cylinders" "displacement" "horsepower" "weight" "acceleration" "year" may have effect on mpg01.

```{r}
set.seed(666)
train <- sample(1:dim(Auto)[1],size =   
                  floor(0.75*dim(Auto)[1]))
#split the data
```

Fit LDA firstly
```{r}
library(MASS)
lda.fit <- lda(mpg01~.-mpg-origin-name,data=Auto,
               subset=train)
lda.pre <- predict(lda.fit,Auto[-train,])$class
#testerror
mean(lda.pre!=Auto[-train,"mpg01"])
```

Fit QDA
```{r}
qda.fit <- qda(mpg01~.-mpg-origin-name,data=Auto,
               subset=train)
qda.pre <- predict(qda.fit,Auto[-train,])$class
#testerror
mean(qda.pre!=Auto[-train,"mpg01"])
```

Fit logistic
```{r}
logistic.fit <- glm(mpg01~cylinders+displacement+horsepower
                    +weight+acceleration+year,data=Auto,
               subset=train,family=binomial(link="logit"))
logistic.pre <- predict(logistic.fit,
                        newdata=as.data.frame(Auto[-train,]),
                        type="response")
logistic.pre <- ifelse(logistic.pre>=0.5,1,0)
#testerror
mean(logistic.pre!=Auto[-train,"mpg01"])
```

Fit KNN
```{r}
library(class)
knn.fit <- list()
for (k in 1:10)
{
  knn.pre <- knn(Auto[train,2:7],test=Auto[-train,2:7],
                 cl=Auto[train,"mpg01"],k=k)
  knn.fit <- c(knn.fit,list(knn.pre))
}
str(knn.fit)
errorrate <- function(x) mean(x!= Auto[-train,"mpg01"])
testerror <- lapply(knn.fit,errorrate)
plot(1:10,testerror,type="b")
```
choose k=3

####Exercise 12
```{r}
power <- function(){print(2^3)}
power()
power2 <- function(x,a){print(x^a)}
power2(3,8)
power2(10,3);power2(8,17);power2(131,3)
power3 <- function(x,a){result=x^a;return(result)}
x <- 1:10
x2 <- power3(x,2)
plot(x,x2,xlab="log(x)",ylab="log(y)",log="xy")
plotPower <- function(x,a)
{
  x.power <- power3(x,a)
  p = plot(x,x.power,xlab="log(x)",ylab="log(y)",log="xy")
  return(p)
}
plotPower(1:10,3)
```

